from requests.exceptions import ConnectionError
import dns.resolver
import threading
import socket
import os
import sys
from socket import gethostbyname
import requests
from bs4 import BeautifulSoup
from termcolor import colored
import json

os.system("clear")
# Fonction pour écrire les résultats dans un fichier JSON
def save_results_to_json(data, filename="report/result_gathring.json"):
    with open(filename, 'a') as f:
        json.dump(data, f, indent=4)
        f.write("\n")


# Ajouter une fonction pour écrire les résultats dans un fichier JSON
def write_result_to_json(result_type, result_data):
    result = {
        "type": result_type,
        "data": result_data
    }
    save_results_to_json(result)

def judul():
    print('''

 \033[91m\033[1m                                                      _____   ____    _____     
 \033[91m\033[1m                                                     / ____| |  _ \  |_   _|    
 \033[91m\033[1m                                                    | |      | |_) |   | |      
 \033[91m\033[1m                                                    | |      |  _ <    | |      
 \033[91m\033[1m                                                    | |____  | |_) |  _| |_     
 \033[91m\033[1m                                                     \_____| |____/  |_____|    
\033[91m\033[1m                
\033[91m\033[1m                    _____        __                           _   _                _____       _   _               _              
\033[91m\033[1m                   |_   _|      / _|                         | | (_)              / ____|     | | | |             (_)             
\033[91m\033[1m                     | |  _ __ | |_ ___  _ __ _ __ ___   __ _| |_ _  ___  _ __   | |  __  __ _| |_| |__   ___ _ __ _ _ __   __ _  
\033[97m\033[1m                     | | | '_ \|  _/ _ \| '__| '_ ` _ \ / _` | __| |/ _ \| '_ \  | | |_ |/ _` | __| '_ \ / _ \ '__| | '_ \ / _` | 
\033[97m\033[1m                    _| |_| | | | || (_) | |  | | | | | | (_| | |_| | (_) | | | | | |__| | (_| | |_| | | |  __/ |  | | | | | (_| | 
\033[97m\033[1m                   |_____|_| |_|_| \___/|_|  |_| |_| |_|\__,_|\__|_|\___/|_| |_|  \_____|\__,_|\__|_| |_|\___|_|  |_|_| |_|\__, | 
\033[97m\033[1m                                                                                                                            __/ | 
 \033[97m\033[1m                                                                                                                          |___/  

{0}'''.format(reset, red, red, white, red, bold))


target = ''

def tampilan_pilihan(target):
    #os.system("clear")
    print("\n\n\n")
    print("\t\t\t*************************************************")
    print("\t\t\t*\t\t\t\t\t\t*")
    print("\t\t\t*\t"+f"{red}[ Web Information Gathering Menu ]{reset}\t*")
    print("\t\t\t*\t"+f"{white}[ Target = {target} ]{reset}\t\t*")
    print("\t\t\t*\t\t\t\t\t\t*")
    print("\t\t\t*************************************************")
    print("\t\t\t*\t\t\t\t\t\t*")
    print("\t\t\t*\t 1)  Banner Grab\t\t\t*")
    print("\t\t\t*\t 2)  Tracerroute\t\t\t*")
    print("\t\t\t*\t 3)  DNS Record\t\t\t\t*")
    print("\t\t\t*\t 4)  Reverse DNS Lookup\t\t\t*")
    print("\t\t\t*\t 5)  Port Scan\t\t\t\t*")
    print("\t\t\t*\t 6)  Subdomain Scan\t\t\t*")
    print("\t\t\t*\t 7)  Extract Page Links\t\t\t*")
    print("\t\t\t*\t 8)  Directory Fuzz\t\t\t*")
    print("\t\t\t*\t 9)  Panel Scan\t\t\t\t*")
    print("\t\t\t*\t 10) HTTP Status Check\t\t\t*")
    #print()
    print("\t\t\t*\t 0)  Quitter\t\t\t\t*")
    print("\t\t\t*\t\t\t\t\t\t*")
    print("\t\t\t*************************************************")
    print("")
    print()


bold = '\033[1m'
underline = '\033[4m'
black = '\033[90m';
red = '\033[91m';
green = '\033[92m';
yellow = '\033[93m';
blue = '\033[94m';
magenta = '\033[95m';
cyan = '\033[96m';
white = '\033[97m'
reset = '\033[0m'

ids = [
    'NONE', 'A', 'NS', 'MD', 'MF', 'CNAME', 'SOA', 'MB', 'MG', 'MR', 'NULL', 'WKS', 'PTR', 'HINFO', 'MINFO', 'MX',
    'TXT', 'RP', 'AFSDB', 'X25', 'ISDN', 'RT', 'NSAP', 'NSAP-PTR', 'SIG', 'KEY', 'PX', 'GPOS', 'AAAA', 'LOC', 'NXT',
    'SRV', 'NAPTR', 'KX', 'CERT', 'A6', 'DNAME', 'OPT', 'APL', 'DS', 'SSHFP', 'IPSECKEY', 'RRSIG', 'NSEC', 'DNSKEY',
    'DHCID', 'NSEC3', 'NSEC3PARAM', 'TLSA', 'HIP', 'CDS', 'CDNSKEY', 'CSYNC', 'SPF', 'UNSPEC', 'EUI48', 'EUI64', 'TKEY',
    'TSIG', 'IXFR', 'AXFR', 'MAILB', 'MAILA', 'ANY', 'URI', 'CAA', 'TA', 'DLV'
]


def banner_grab(url):
    global http_target
    if "http://" not in url:
        http_target = "http://" + url
    banner = requests.get(http_target)
    banners = banner.headers
    banners_result = str(banners).replace("{", "").replace("}", "").replace("': '", ": ").replace("', '", ",\n")
    return banners_result
def tracerroute(target, api_key):
    url = 'https://api.hackertarget.com/mtr/?q={}&apikey={}'.format(target, api_key)
    try:
        tracerroute_req = requests.get(url)
        tracerroute_respon = tracerroute_req.text
        result = """{0}""".format(str(tracerroute_respon))
        return result
    except Exception as e:
        return "Error: {}".format(str(e))


def dns_record_scanner(hostname, ids, dns_record_list):
    try:
        respon = dns.resolver.query(hostname, ids)
        for data in respon:
            ids = str(ids);
            data = str(data)
            dns_record_list.append(str(ids + ' : ' + data))
    except Exception:
        pass


def dns_record(hostname):
    global t
    dns_record_list = []
    for a in ids:
        t = threading.Thread(target=dns_record_scanner, args=(hostname, a, dns_record_list))
        t.start()
    t.join()
    result = dns_record_list
    return result


def reverse_ip_lookup(hostname):
    req = requests.get('https://api.hackertarget.com/reverseiplookup/?q={}'.format(hostname))
    respon = req.text
    result = """{0}""".format(str(respon))
    return result
def TCP_connect(ip, port, delay, output):
    TCPsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    TCPsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    TCPsock.settimeout(delay)
    try:
        TCPsock.connect((ip, port))
        output[port] = 'Open'
    except:
        output[port] = ''


def port_scan(hostname, port_end):
    port_scan_list = []
    threads = []
    output = {}
    delay = 10
    for i in range(port_end + 1):
        t = threading.Thread(target=TCP_connect, args=(hostname, i, delay, output))
        threads.append(t)
    for i in range(port_end + 1):
        threads[i].start()
    for i in range(port_end + 1):
        threads[i].join()
    for i in range(port_end + 1):
        if output[i] == 'Open':
            port_scan_list.append('# Port Open - ' + str(i))
    return port_scan_list


def subdomain_scanner(web, st_200, st_301, st_302, st_403, st_404):
    web = 'http://' + web
    try:
        req = requests.get(web)
        req_code = req.status_code
        if req_code == 200:
            st_200.append(web)
            print("{}{} ->[200]{}".format(green, web, reset))
        elif req_code == 301:
            st_301.append(web)
            print("{}{} ->[301]{}".format(blue, web, reset))
        elif req_code == 302:
            st_302.append(web)
            print("{}{} ->[302]{}".format(cyan, web, reset))
        elif req_code == 403:
            st_403.append(web)
            print("{}{} ->[403]{}".format(red, web, reset))
        else:
            st_404.append(web)
            print("{}{} ->[404]{}".format(yellow, web, reset))
    except ConnectionError:
        pass


def subdomain_scan(hostname, subdomain_list):
    global t
    st_200 = []
    st_301 = []
    st_302 = []
    st_403 = []
    st_404 = []
    ss_urls = []
    subdomain_list = open(subdomain_list, 'r').read().splitlines()
    for i in subdomain_list:
        ss_urls.append(i + '.' + hostname)
    for i in ss_urls:
        t = threading.Thread(target=subdomain_scanner, args=(i, st_200, st_301, st_302, st_403, st_404))
        t.start()
    t.join()
def fast_crawl(url):
    if "http://" not in url:
        url = "http://" + url
    global list_direct, url_access, url_source
    ip = url.strip("https://www.")
    print("Domain:", url)
    print("IP:", gethostbyname(ip))
    list_direct = []
    url_strip = url.strip("https://www.")
    headers = {"user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0"}
    list_direct.append(url)
    url_request = requests.get(url, headers=headers)
    url_source = BeautifulSoup(url_request.content, "html.parser")
    for link in url_source.find_all("a"):
        link_pure = link.get("href")
        try:
            if "#" in link_pure or "../" in link_pure or "facebook.com" in link_pure or "@" in link_pure:
                pass
            else:
                if "http" not in link_pure and "https" not in link_pure and url_strip not in link_pure:
                    try:
                        first_req = requests.get(url + link_pure)
                        if first_req.status_code == 200:
                            print(colored("================================================================", "green"))
                            print(colored("Url:", "green"), url + link_pure)
                            print(colored("Request:", "green"), first_req.status_code)
                            print(colored("================================================================", "green"))
                            list_direct.append(url + link_pure)
                        else:
                            pass
                    except requests.exceptions.ConnectionError:
                        pass
                else:
                    if "http" in link_pure or "https" in link_pure and url_strip in link_pure:
                        try:
                            sec_req = requests.get(link_pure)
                            if sec_req.status_code == 200:
                                if sec_req.url not in list_direct:
                                    print(colored("================================================================",
                                                  "green"))
                                    print(colored("Url:", "green"), link_pure)
                                    print(colored("Request:", "green"), sec_req.status_code)
                                    print(colored("================================================================",
                                                  "green"))
                                    list_direct.append(link_pure)
                            else:
                                pass
                        except requests.exceptions.ConnectionError:
                            pass
                    elif "http" not in link_pure or "https" not in link_pure and url_strip in link_pure:
                        try:
                            third_req = requests.get("http://" + link_pure)
                            if third_req.status_code == 200:
                                if third_req.url not in list_direct:
                                    print(colored("================================================================",
                                                  "green"))
                                    print(colored("Url:", "green"), third_req.url)
                                    print(colored("Request:", "green"), third_req.status_code)
                                    print(colored("================================================================",
                                                  "green"))
                                    list_direct.append("http://" + link_pure)
                            else:
                                pass
                        except requests.exceptions.ConnectionError:
                            pass
                    else:
                        try:
                            fourth_req = requests.get(link_pure)
                            if fourth_req.status_code == 200:
                                if fourth_req.url not in list_direct:
                                    print(colored("================================================================",
                                                  "green"))
                                    print(colored("Url:", "green"), fourth_req.url)
                                    print(colored("Request:", "green"), fourth_req.status_code)
                                    print(colored("================================================================",
                                                  "green"))
                                    list_direct.append(fourth_req.url)
                            else:
                                pass
                        except requests.exceptions.ConnectionError:
                            pass
        except:
            pass
    for url_form_list in list_direct:
        sec_url_request = requests.get(url_form_list)
        soup = BeautifulSoup(sec_url_request.content, "html.parser")
        for sec_link in soup.find_all("a"):
            sec_link = sec_link.get("href")
            try:
                if "#" in sec_link or "./" in sec_link:
                    pass
                else:
                    if url_strip not in sec_link:
                        pass
                    else:
                        if "http" not in sec_link or "https" not in sec_link and url_strip in sec_link:
                            try:
                                five_req = requests.get("http://" + sec_link)
                                if five_req.status_code == 200:
                                    if five_req.url not in list_direct:
                                        print(
                                            colored("================================================================",
                                                    "green"))
                                        print(colored("Url:", "green"), five_req.url)
                                        print(colored("Request:", "green"), five_req.status_code)
                                        print(
                                            colored("================================================================",
                                                    "green"))
                                        list_direct.append(five_req.url)
                                else:
                                    pass
                            except:
                                pass
                        else:
                            try:
                                six_req = requests.get(sec_link)
                                if six_req.status_code == 200:
                                    if six_req.url not in list_direct:
                                        print(
                                            colored("================================================================",
                                                    "green"))
                                        print(colored("Url:", "green"), six_req.url)
                                        print(colored("Request:", "green"), six_req.status_code)
                                        print(
                                            colored("================================================================",
                                                    "green"))
                                        list_direct.append(six_req.url)
                                else:
                                    pass
                            except:
                                pass
            except:
                pass
    return list_direct
def directory_scanner(url_list, directory_fuzz1, directory_fuzz2, directory_fuzz3):
    try:
        req = requests.get(url_list)
        if req.status_code == 200:
            directory_fuzz1.append(url_list)
            print("{}{} ->[200]{}".format(white, url_list, reset))
        elif req.status_code == 301 or req.status_code == 302:
            directory_fuzz2.append(url_list)
            print("{}{} ->[301/302]{}".format(white, url_list, reset))
        elif req.status_code == 403:
            directory_fuzz3.append(url_list)
            print("{}{} ->[403]{}".format(red, url_list, reset))
        else:
            print("{}{} ->[404]{}".format(red, url_list, reset))

    except requests.exceptions.RequestException as e:
        # Gestion spécifique des exceptions de requête
        print(f"Erreur de requête pour {url_list}: {e}")
    except Exception as e:
        # Gestion générale des exceptions
        print(f"Erreur inattendue pour {url_list}: {e}")


def file_scanner(url_list, file_fuzz1, file_fuzz2, file_fuzz3):
    try:
        req = requests.get(url_list)
        if req.status_code == 200:
            file_fuzz1.append(url_list)
            print("{}{} ->[200]{}".format(white, url_list, reset))
        elif req.status_code == 301 or req.status_code == 302:
            file_fuzz2.append(url_list)
            print("{}{} ->[301/302]{}".format(white, url_list, reset))
        elif req.status_code == 403:
            file_fuzz3.append(url_list)
            print("{}{} ->[403]{}".format(red, url_list, reset))
        else:
            print("{}{} ->[404]{}".format(red, url_list, reset))
    except:
        pass


def directory_fuzz(url, directory_list):
    if "http://" not in url:
        url = "http://" + url
    global t
    directory_fuzz1 = []
    directory_fuzz2 = []
    directory_fuzz3 = []
    with open(directory_list, 'r') as file:
        directories = file.read().splitlines()
    url_list = []
    threads = []

    # Ajout du "/" si nécessaire à chaque élément de la liste
    for directory in directories:
        if not directory.startswith("/"):
            directory = "/" + directory
        url_list.append(url + directory)

    # Création et démarrage des threads pour le scanner de répertoires
    for i in url_list:
        try:
            response = requests.get(url)

            # Si la réponse est réussie (code 200), afficher l'URL
            if response.status_code == 200:
                print(f"URL valide : {url}")
            t = threading.Thread(target=directory_scanner, args=(i, directory_fuzz1, directory_fuzz2, directory_fuzz3))
            threads.append(t)
            t.start()

            # Si une exception est levée (par exemple, URL invalide), passer à l'URL suivante
        except Exception as e:
            pass

    # Attendre que tous les threads se terminent
    for thread in threads:
        thread.join()

    return {
        "directory_fuzz1": directory_fuzz1,
        "directory_fuzz2": directory_fuzz2,
        "directory_fuzz3": directory_fuzz3
    }
def admin_panel(url):
    if "http://" not in url:
        url = "http://" + url
    file_format = open("link.txt", "r")
    found_urls = []
    not_found_urls = []

    try:
        for link in file_format:
            Purl = url + "/" + link.strip()
            if Purl is None:
                exit()
            req_link = requests.get(Purl)
            if req_link.status_code == 200:
                print(colored("[+]Found: ", "green"), Purl)
                found_urls.append(Purl)
            else:
                print(colored("[-]Not Found: ", "red"), Purl)
                not_found_urls.append(Purl)
    except requests.exceptions.ConnectionError:
        pass

    return {
        "found_urls": found_urls,
        "not_found_urls": not_found_urls
    }
def httplive(url):
    if "http://" not in url:
        url = "http://" + url
    global live
    live = None
    try:
        request_live = requests.get(url)
        if request_live.status_code == 200:
            print(colored("Http Live : ", "green"), url)
            live = 1
        else:
            print(colored("Http Down : ", "red"), url)
            live = 0
    except requests.exceptions.ConnectionError:
        print(colored("Http Down : ", "red"), url)
        live = 0

    return {"url": url, "live": live}
def main():
    # judul()
    tampilan_pilihan(target)
    pilihan = int(input('\n{}CBI> {}{}'.format(red, white, reset)))
    print('\n')
    if pilihan == 1:
        print("{}[ ### Banner Grab ###]{}\n".format(red, reset))
        hasil = banner_grab(http_target)
        hasil = str(hasil).replace("'", "")
        print("{}{}".format(white, hasil))
        write_result_to_json("Banner Grab", hasil)

    elif pilihan == 2:
        print("{}[ ### Tracerroute ### ]{}\n".format(red, reset))
        api_key = input("Veuillez saisir votre API key : ")
        get_ip = socket.gethostbyname(target)
        hasil = tracerroute(get_ip, api_key)
        print("{}{}".format(white, hasil))
        write_result_to_json("Traceroute", hasil)

    elif pilihan == 3:
        print("{}[ ### DNS Record ### ]{}\n".format(red, reset))
        hasil = dns_record(target)
        hasil = str(hasil).replace("[", " ").replace("'", "").replace(",", "\n").replace("]", "")
        print("{}{}".format(white, hasil))
        write_result_to_json("DNS Record", hasil)

    elif pilihan == 4:
        print("{}[ ### Reverse DNS Lookup ### ]{}\n".format(red, reset))
        ip_file = input("Enter the IP list file name: ")
        hasil=os.system(f"python3 reverseDNS.py -il {ip_file}")
        write_result_to_json("Reverse DNS Lookup", hasil)

    elif pilihan == 5:
        print("{}[ ### Port ### ]{}\n".format(red, reset))
        port_end = int(input("{}Port End :{}".format(yellow, blue)))
        hasil = port_scan(target, port_end)
        write_result_to_json("Port Scan", hasil)
        hasil = str(hasil).replace("[", "").replace("]", "").replace("'", "").replace(",", "\n")
        print("{}{}".format(white, hasil))


    elif pilihan == 6:
        print("{}[ ### Subdomain Scan ### ]{}\n".format(red, reset))
        subdomain_list = input("{}File Subdomain List : {}".format(yellow, blue))
        subdomain_scan(target, subdomain_list)
        write_result_to_json("Subdomain Scan", subdomain_scan(target, subdomain_list))

    elif pilihan == 7:
        print("{}[ ### Extract Page Links ### ]{}\n".format(red, reset))
        hasil = fast_crawl(target)
        #print("{}{}".format(white,hasil))
        write_result_to_json("Extract Page Links", hasil)
    elif pilihan == 8:
        print("{}[ ### Directory Fuzz ### ]{}\n".format(red, reset))
        directory_list = input("{}File Directory List : {}".format(yellow, blue))
        hasil = directory_fuzz(http_target, directory_list)
        write_result_to_json("Directory Fuzz", hasil)
    elif pilihan == 9:
        print("{}[ ### Panel Scan ### ]{}\n".format(red, reset))
        hasil = admin_panel(target)
        # print("{}{}".format(white,hasil))
        write_result_to_json("Panel Scan", hasil)
    elif pilihan == 10:
        print("{}[ ### HTTP Status Check ### ]{}\n".format(red, reset))
        hasil = httplive(target)
        # print("{}{}".format(white, hasil))
        write_result_to_json("HTTP Status Check", hasil)
    elif pilihan == 0:
        print("Quitter")
        sys.exit()

    else:
        print("{}Nothing!".format(red))
        os.system('clear')
        exit()
    print("\n\n")
    #print("\n\033[96mRésultat sauvegardé dans 'report/result_gathring.json'\n")


if __name__ == '__main__':
    judul()
    target = input('{}Set Target (IP) : {}'.format(red, white))
    if not target:
        print("Veuillez fournir une valeur pour la cible.")
        exit()  # Quitter le programme si aucun target n'est fourni
    http_target = target
    # if "http://" not in http_target:
    # http_target = "http://"+target
    os.system('clear')
    ulang = ''
    while ulang != 'n':
        main()
        ulang = input("\n{}Back?(y/n) : {}".format(red, white))
        os.system('clear')